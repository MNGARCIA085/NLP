{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd08487",
   "metadata": {},
   "source": [
    "# <center><font color='blue'>SkimLit</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9abd523",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [1 - Objetivos](#1)\n",
    "- [2 - Librerías necesarias](#2)\n",
    "- [3 - Carga y visualización de datos](#3)\n",
    "- [4 - Pre-procesamiento de datos](#4)\n",
    "    - [4.1. - Datos faltantes](#4.1)\n",
    "    - [4.2. - Data Categóricos](#4.2)\n",
    "    - [4.3. - Balanceo de clases](#4.3)\n",
    "    - [4.4. - Pre-Procesamiento especial para NLP](#4.4)\n",
    "- [5 - Modelos](#5)\n",
    "    - [5.1. - Modelo 1](#5.1)\n",
    "    - [5.2. - Modelo 2](#5.2)\n",
    "    - [5.3. - Modelo 3](#5.3)\n",
    "    - [5.4. - Modelo 4](#5.4)\n",
    "    - [5.4. - Modelo 5](#5.5)\n",
    "    - [5.4. - Modelo 6](#5.6)\n",
    "    - [5.4. - Modelo 7](#5.7)\n",
    "- [6 - Comparando los modelos y eligiendo el mejor](#6)\n",
    "- [7 - Ajuste de hiperparámetros](#7)\n",
    "- [8 - Predicciones con el modelo final](#8)\n",
    "- [9 - Guardando el modelo](#9)\n",
    "- [10 - Conclusiones](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cea80",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <b> <font color='blue'> 1. Objectives </font> </b>\n",
    "Build an NLP model to make reading medical abstracts easier.\n",
    "\n",
    "The paper we're replicating (the source of the dataset that we'll be using) is available here: https://arxiv.org/abs/1710.06071\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22725ba",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <b> <font color='blue'> 2. Setup </font> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f10078",
   "metadata": {},
   "source": [
    "What we are looking for is to associate a specific label (objective, background, result...) with a given sentence (composed of many words), so it is a many-to-one problem.\n",
    "\n",
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa9711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# que no se impriman info y warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ab0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, models, Sequential, losses\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0455e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff93be",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <b> <font color='blue'> 3.  Data Loading and Visualization </font> </b>\n",
    "\n",
    "Let's download the data.\n",
    "\n",
    "We can do so from the authors GitHub: https://github.com/Franck-Dernoncourt/pubmed-rct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
    "!dir pubmed-rct #ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30787a52",
   "metadata": {},
   "source": [
    "There are 2 datasets, one with 20000 examples (usefull for the initial tests) and another one with 200k examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9223d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files are in the PubMed_20K dataset\n",
    "!dir pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start our experiments using the 20k dataset with numbers replaced by \"@\" sign\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all of the filenames in the target directory\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e43b1",
   "metadata": {},
   "source": [
    "So with that in mind, let's write a function to read in all of the lines of a target text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d60e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "  \"\"\"\n",
    "  Reads filename (a text filename) and returns the lines of text as a list.\n",
    "\n",
    "  Args:\n",
    "    filename: a string containing the target filepath.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings with one string per line from the target filename.\n",
    "  \"\"\"\n",
    "  with open(filename, \"r\") as f:\n",
    "    return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5532a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the training lines and see some of them\n",
    "train_lines = get_lines(data_dir+\"train.txt\") # read the lines with the training file\n",
    "\n",
    "to_show = 15\n",
    "train_lines[:to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57d4e9",
   "metadata": {},
   "source": [
    "We see that the abstracts:\n",
    "\n",
    "- Start with \"###\n",
    "- Followed by an ID and a newline character (\\n)\n",
    "- Each sentence has a label (for example RESULTS, METHODS..) (starting with the label and then \\t)\n",
    "- The end is indicated by a newline charecter (\\n).\n",
    "\n",
    "<br>\n",
    "We need a function to separate the text from the labels and the different abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d75e7",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <b> <font color='blue'> 4.  Data pre-processing </font> </b>\n",
    "\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### <b> <font color='#1F618D'> 4.1. Formatting our data </font> </b>\n",
    "\n",
    "We want that our data looks like this:\n",
    "\n",
    "```\n",
    "[{'line_number': 0,\n",
    "   'target': 'BACKGROUND',\n",
    "   'text': \"Emotional eating is associated with overeating and the development of obesity .\\n\"\n",
    "   'total_lines': 11},\n",
    "   ...]\n",
    "```\n",
    "\n",
    "total lines it's the number of lines in the abstract (that we want to classify sequentially)\n",
    "\n",
    "Let's write a function which turns each of our datasets into the above format so we can continue to prepare our data for modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "  \"\"\"\n",
    "  Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "  Takes in filename, reads it contents and sorts through each line,\n",
    "  extracting things like the target label, the text of the sentnece,\n",
    "  how many sentences are in the current abstract and what sentence\n",
    "  number the target line is.\n",
    "  \"\"\"\n",
    "  input_lines = get_lines(filename) # get all lines from filename\n",
    "  abstract_lines = \"\" # create an empty abstract\n",
    "  abstract_samples = [] # create an empty list of abstracts\n",
    "\n",
    "  # Loop through each line in the target file\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"): # check to see if the is an ID line\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
    "\n",
    "    elif line.isspace(): # check to see if line is a new line\n",
    "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "      # Iterate through each line in a single abstract and count them at the same time\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {} # create an empty dictionary for each line\n",
    "        target_text_split = abstract_line.split(\"\\t\") # split target label from text \n",
    "        line_data[\"target\"] = target_text_split[0] # get target label\n",
    "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are there in the target abstract? (start from 0)\n",
    "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "\n",
    "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "      abstract_lines += line\n",
    "  \n",
    "  return abstract_samples\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4548271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from file and preprocess it\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation dataset\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "\n",
    "print(len(train_samples), len(val_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf54a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first abstract of our training data\n",
    "train_samples[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f48bcd",
   "metadata": {},
   "source": [
    "Let's create dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855ab61",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### <b> <font color='#1F618D'> 4.2. More visualization </font> </b>\n",
    "\n",
    "#### Number of classes and class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = train_df['target'].nunique()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bac68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a250b7c",
   "metadata": {},
   "source": [
    "#### Total lines distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the length of different lines\n",
    "train_df.total_lines.plot.hist(); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008a98b",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538361e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd78189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 5 lines of training sentences\n",
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b20a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long is each sentence on average?\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7da91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the distribution look like?\n",
    "\n",
    "# Crear el histograma\n",
    "n, bins, patches = plt.hist(sent_lens, bins=5, edgecolor='black')\n",
    "\n",
    "# Colores para cada barra\n",
    "colors = ['blue', 'cyan', 'green', 'purple', 'orange']\n",
    "\n",
    "# Asignar un color a cada barra\n",
    "for patch, color in zip(patches, colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f90161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long of a sentence lenght covers 95% of examples?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length in the training set\n",
    "max(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdc981",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### <b> <font color='#1F618D'> 4.3. Categorical Data </font> </b>\n",
    "\n",
    "We will use one-hot encoding for our targets, since there are no ordinal relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # we want non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# here there is no fit, we fit with the training data only\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# check what one hot encoded labels look like\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55811dde",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### <b> <font color='#1F618D'> 4.4. Pre-processing for NLP </font> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a3d1a",
   "metadata": {},
   "source": [
    "#### Create text vectorizer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aef1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are in our vocab? (taken from table 2 in: https://arxiv.org/pdf/1710.06071.pdf)\n",
    "max_tokens = 68000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "# Create text vectorizer\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
    "                                    output_sequence_length=output_seq_len) # desired output length of vectorized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d54233",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text: {text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05a8b4",
   "metadata": {},
   "source": [
    "Let's observe that it pads with zeros up to the specified output sequence length (output_seq_length, which is 55 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words in our training vocabulary\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
    "print(f\"Most common words in the vocab: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words in the vocab: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc65f7",
   "metadata": {},
   "source": [
    "<b> \n",
    "We will apply it later as the first layer of the model after obtaining the input. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce2aee",
   "metadata": {},
   "source": [
    "<a name=\"4.5\"></a>\n",
    "### <b> <font color='#1F618D'> 4.5. Creating tensorflow datasets </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b190fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73558ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(f\"Text: {x}\\n\")\n",
    "    print(f\"Label: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c509c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefected datasets\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0a4c5",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <b> <font color='blue'> 5. Models </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save results and compare\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "INPUT_SHAPE=(1,)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES=num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da8219",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### <b> <font color='#1F618D'> 5.1. Embedding layer </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
    "                               output_dim=128, # Note: different embedding sizes result in drastically differnt \n",
    "                                               #numbers of parameters to train\n",
    "                               mask_zero=True, # use masking to handle variable sequence lengths (save space),\n",
    "                               name=\"token_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n {target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n {vectorized_sentence}\\n\")\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n {embedded_sentence}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f492e0b",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### <b> <font color='#1F618D'> 5.2. Trying different models </font> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8b571",
   "metadata": {},
   "source": [
    "<a name=\"5.2.1\"></a>\n",
    "### <b> <font color='#5499C7'> 5.2.1. Model 1: Conv1D </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a882d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1(name, input_shape = INPUT_SHAPE, num_classes = NUM_CLASSES):\n",
    "    inputs = layers.Input(shape=input_shape,dtype=tf.string)\n",
    "    x = text_vectorizer(inputs)\n",
    "    x = token_embed(x)\n",
    "    x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs,outputs,name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model_1('model_1')\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cf48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), # one-hot encoded labels\n",
    "    metrics=['accuracy', 'Precision', 'Recall']\n",
    ")\n",
    "\n",
    "\n",
    "history_model_1 = model_1.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "    epochs=3,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=int(0.1 * len(valid_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "model_1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (our model predicts prediction probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs, model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pred probs to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcafd19",
   "metadata": {},
   "source": [
    "<a name=\"5.2.2\"></a>\n",
    "### <b> <font color='#5499C7'> 5.2.2. Model 2: Feature extraction with pre-trained token embeddings </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48577412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc098929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the pretrained embedding on a random sentence \n",
    "random_train_sentence = random.choice(train_sentences)\n",
    "print(f\"Random sentence:\\n {random_train_sentence}\")\n",
    "use_embedded_sentence = tf_hub_embedding_layer([random_train_sentence])\n",
    "print(f\"Setence after embedding:\\n{use_embedded_sentence[0][:30]}\\n\")\n",
    "print(f\"Length of sentence embedding: {len(use_embedded_sentence[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2(name, input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=tf.string)\n",
    "    x = tf_hub_embedding_layer()(inputs)\n",
    "    x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes,activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs,outputs,name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_2 = build_model_1('model_2_USE_feature_extractor')\n",
    "    \n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5454d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), # one-hot encoded labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history_model_2 = model_2.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "    epochs=3,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=int(0.1 * len(valid_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with feature extraction model\n",
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "model_2_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prediction probabilities found with feature extraction model to labels\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "model_2_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cf55e",
   "metadata": {},
   "source": [
    "<a name=\"5.2.3\"></a>\n",
    "### <b> <font color='#5499C7'> 5.2.3. Model 3: Conv1D with character embeddings </font> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f2855",
   "metadata": {},
   "source": [
    "#### Creating a character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "train_chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7152ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the average character length?\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc861543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el histograma\n",
    "n, bins, patches = plt.hist(char_lens, bins=5, edgecolor='black')\n",
    "\n",
    "# Colores para cada barra\n",
    "colors = ['blue', 'cyan', 'green', 'purple', 'orange']\n",
    "\n",
    "# Asignar un color a cada barra\n",
    "for patch, color in zip(patches, colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5dfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfe955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all keyboard characters\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')\n",
    "\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    # standardize=None, # set standardization to \"None\" if you want to leave punctuation in\n",
    "                                    name=\"char_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt character vectorizer to training character\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4107f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check character vocab stats\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Charified text:\\n {random_train_chars}\")\n",
    "print(f\"\\nLength of random_train_chars: {len(random_train_chars.split())}\")\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"\\nVectorized chars:\\n {vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b29167",
   "metadata": {},
   "source": [
    "#### Creating a character-level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2eeb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char embedding layer\n",
    "char_embed = layers.Embedding(input_dim=len(char_vocab), # number of different characters\n",
    "                              output_dim=25, # this is the size of the char embedding in the paper: https://arxiv.org/pdf/1612.05251.pdf (Figure 1)\n",
    "                              mask_zero=True,\n",
    "                              name=\"char_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our character embedding layer\n",
    "print(f\"Charified text:\\n {random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n {char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b0288",
   "metadata": {},
   "source": [
    "Each sentence has length 290 and the size of the embedding is 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5549bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char level datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_3(name, input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES):\n",
    "    inputs = layers.Input(shape=input_shape,dtype=tf.string)\n",
    "    x = char_vectorizer(inputs)\n",
    "    x = char_embed(x)\n",
    "    x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x) # try MaxPooling!!\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs,outputs,name=name)\n",
    "    return model\n",
    "\n",
    "    \n",
    "model_3 = build_model_3('model_3')\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\", 'Precision', 'Recall'])\n",
    "\n",
    "# Fit the model on chars only\n",
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1*len(train_char_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1*len(val_char_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(test_char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da18a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with character model only\n",
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f012b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction probabilities to class labels\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527cd38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
