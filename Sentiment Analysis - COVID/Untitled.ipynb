{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22e347b",
   "metadata": {},
   "source": [
    "# <center><font color='blue'>SENTIMENT ANALYSIS: COVID</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad676c05",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [1 - Objetivos](#1)\n",
    "- [2 - Librerías necesarias](#2)\n",
    "- [3 - Carga y visualización de datos](#3)\n",
    "- [4 - Pre-procesamiento de datos](#4)\n",
    "    - [4.1. - Datos faltantes](#4.1)\n",
    "    - [4.2. - Data Categóricos](#4.2)\n",
    "    - [4.3. - Balanceo de clases](#4.3)\n",
    "    - [4.4. - Pre-Procesamiento especial para NLP](#4.4)\n",
    "- [5 - Modelos](#5)\n",
    "    - [5.1. - Modelo 1](#5.1)\n",
    "    - [5.2. - Modelo 2](#5.2)\n",
    "    - [5.3. - Modelo 3](#5.3)\n",
    "    - [5.4. - Modelo 4](#5.4)\n",
    "    - [5.4. - Modelo 5](#5.5)\n",
    "    - [5.4. - Modelo 6](#5.6)\n",
    "    - [5.4. - Modelo 7](#5.7)\n",
    "- [6 - Comparando los modelos y eligiendo el mejor](#6)\n",
    "- [7 - Ajuste de hiperparámetros](#7)\n",
    "- [8 - Predicciones con el modelo final](#8)\n",
    "- [9 - Guardando el modelo](#9)\n",
    "- [10 - Conclusiones](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9146a9",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <b> <font color='blue'> 1. Objectives </font> </b>\n",
    "\n",
    "Practice with a natural language processing problem.\n",
    "<br>\n",
    "Here, given a set of tweets, analyze whether the sentiment is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b0993",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <b> <font color='blue'> 2. Setup </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61752a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not print info and warning messages\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf1442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 20:50:00.056418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-10 20:50:00.056468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-10 20:50:00.057664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476eaa3b",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <b> <font color='blue'> 3. Data loading and visualization </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fda961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona_NLP_test.csv  Corona_NLP_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb2d11",
   "metadata": {},
   "source": [
    "We have 2 datasets, one for training and one for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac1707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/Corona_NLP_train.csv',encoding='latin-1')\n",
    "test_data = pd.read_csv('data/Corona_NLP_test.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5977366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3805</td>\n",
       "      <td>48757</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3807</td>\n",
       "      <td>48759</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                        OriginalTweet  \\\n",
       "0                                                                                                                                                                                                                     @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8   \n",
       "1                                                                                       advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order   \n",
       "2                                                                                                                                                                                                 Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P   \n",
       "3   My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j   \n",
       "4  Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n   \n",
       "5                                                                          As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU   \n",
       "6                                                                                                                                                Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO   \n",
       "7                                                                                                                                                                                                       Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ   \n",
       "8                                            Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i   \n",
       "9                                                         For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona   \n",
       "\n",
       "            Sentiment  \n",
       "0             Neutral  \n",
       "1            Positive  \n",
       "2            Positive  \n",
       "3            Positive  \n",
       "4  Extremely Negative  \n",
       "5            Positive  \n",
       "6            Positive  \n",
       "7             Neutral  \n",
       "8            Positive  \n",
       "9            Negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some of the training data\n",
    "pd.set_option('display.max_colwidth', None) # que No recorte el texto\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72200c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training data: (rows, cols) = (41157, 6)\n",
      " Training data type:  <class 'pandas.core.frame.DataFrame'>\n",
      " Test data: (rows, cols) = (3798, 6)\n",
      " Test data type:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(f\" Training data: (rows, cols) = {train_data.shape}\")\n",
    "print(f\" Training data type:  {type(train_data)}\")\n",
    "print(f\" Test data: (rows, cols) = {test_data.shape}\")\n",
    "print(f\" Test data type:  {type(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e6aa7",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <b> <font color='blue'> 4. Data Pre-processing </font> </b>\n",
    "\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### <b> <font color='#5499C7'> 4.1. Missing data </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c21ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data train:\n",
      " UserName            0\n",
      "ScreenName          0\n",
      "Location         8590\n",
      "TweetAt             0\n",
      "OriginalTweet       0\n",
      "Sentiment           0\n",
      "dtype: int64 \n",
      "\n",
      "Missing data test:\n",
      " UserName           0\n",
      "ScreenName         0\n",
      "Location         834\n",
      "TweetAt            0\n",
      "OriginalTweet      0\n",
      "Sentiment          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing data train:\\n {train_data.isnull().sum()} \\n')\n",
    "print(f'Missing data test:\\n {test_data.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4ccd4",
   "metadata": {},
   "source": [
    "There are no missing data in the columns we care about (Original Tweet and Sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04a1ec",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### <b> <font color='#5499C7'> 4.2. Feature Extraction </font> </b>\n",
    "\n",
    "Our features will be the column \"OriginalTweet\" and our target the column \"Sentiment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196b1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['OriginalTweet']\n",
    "y_train = train_data['Sentiment']\n",
    "X_test = test_data['OriginalTweet']\n",
    "y_test = test_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104db9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60a633",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### <b> <font color='#5499C7'> 4.3. Categorical data </font> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131c481",
   "metadata": {},
   "source": [
    "Our target is categorical, lets see the different options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82bd1f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8891f9",
   "metadata": {},
   "source": [
    "Lets see if there are other options in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11860104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Extremely Negative', 'Positive', 'Extremely Positive', 'Negative',\n",
       "       'Neutral'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23969648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are unique values the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.array_equal(np.sort(y_train.unique()), np.sort(y_test.unique()))\n",
    "print(f\"Are unique values the same? {are_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9789409",
   "metadata": {},
   "source": [
    "We are not so interested in whether a sentiment is positive or extremely positive, therefore we will rename the label extremely positive to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1523930",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace('Extremely Positive', 'Positive').replace('Extremely Negative', 'Negative')\n",
    "y_test = y_test.replace('Extremely Positive', 'Positive').replace('Extremely Negative', 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e320e35",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ee3c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'Positive' 'Negative'] ['Negative' 'Positive' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique(), y_test.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb4f2c",
   "metadata": {},
   "source": [
    "We will not consider that there is an ordinal relationship, therefore we will encode the labels using one-hot encoding.\n",
    "\n",
    "Remember that we need to apply the same encoding to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bd25c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded train set:\n",
      "       sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
      "0                     0.0                1.0                 0.0\n",
      "1                     0.0                0.0                 1.0\n",
      "2                     0.0                0.0                 1.0\n",
      "3                     0.0                0.0                 1.0\n",
      "4                     1.0                0.0                 0.0\n",
      "...                   ...                ...                 ...\n",
      "41152                 0.0                1.0                 0.0\n",
      "41153                 1.0                0.0                 0.0\n",
      "41154                 0.0                0.0                 1.0\n",
      "41155                 0.0                1.0                 0.0\n",
      "41156                 1.0                0.0                 0.0\n",
      "\n",
      "[41157 rows x 3 columns]\n",
      "\n",
      "One-hot encoded test set:\n",
      "      sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
      "0                    1.0                0.0                 0.0\n",
      "1                    0.0                0.0                 1.0\n",
      "2                    0.0                0.0                 1.0\n",
      "3                    1.0                0.0                 0.0\n",
      "4                    0.0                1.0                 0.0\n",
      "...                  ...                ...                 ...\n",
      "3793                 0.0                0.0                 1.0\n",
      "3794                 1.0                0.0                 0.0\n",
      "3795                 0.0                1.0                 0.0\n",
      "3796                 1.0                0.0                 0.0\n",
      "3797                 0.0                0.0                 1.0\n",
      "\n",
      "[3798 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the train set\n",
    "encoder.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform both train and test sets\n",
    "one_hot_train = encoder.transform(y_train.values.reshape(-1, 1))\n",
    "one_hot_test = encoder.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Convert to DataFrame for easier inspection\n",
    "y_train = pd.DataFrame(one_hot_train, columns = encoder.get_feature_names_out(['sentiment']))\n",
    "y_test = pd.DataFrame(one_hot_test, columns = encoder.get_feature_names_out(['sentiment']))\n",
    "\n",
    "print(\"One-hot encoded train set:\")\n",
    "print(y_train)\n",
    "print(\"\\nOne-hot encoded test set:\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993b3ad",
   "metadata": {},
   "source": [
    "In this example, handle_unknown='ignore' is used to ensure that any categories present in the test set but not in the train set are handled gracefully. This is useful when you have categories in the test set that were not seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67999a1f",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### <b> <font color='#5499C7'> 4.4. Class balance </font> </b>\n",
    "\n",
    "Let's see if the classes are balanced.\n",
    "\n",
    "\n",
    "-> arriba de la codificación!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43eab73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555f29a",
   "metadata": {},
   "source": [
    "<a name=\"4.5\"></a>\n",
    "### <b> <font color='#5499C7'> 4.5. Special pre-processing for NLP </font> </b>\n",
    "\n",
    "Let's preprocess the text from OriginalTweet, for which:\n",
    "\n",
    "- We will remove stop words.\n",
    "- We will remove certain special characters, like \"@\".\n",
    "- We will apply lemmatization.\n",
    "\n",
    "\n",
    "<b>Note:</b> We will also remove punctuation, convert everything to lowercase, and tokenize later using TextVectorization.\n",
    "\n",
    "We will download and print to see the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f38d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b80743",
   "metadata": {},
   "source": [
    "We will remove the stop words and apply lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad234b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Inicializar lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para quitar palabras de parada y lematizar un texto\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Aplicar la función a la columna 'OriginalTweet' del dataset, tanto en train como test\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7f186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12968589",
   "metadata": {},
   "source": [
    "<a name=\"4.6\"></a>\n",
    "### <b> <font color='#5499C7'> 4.6. Train/validation split </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0608250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "#train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b2db564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame de pandas en un objeto tf.data.Dataset\n",
    "# armo según lo que me interesa\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, \n",
    "                                              y_train))\n",
    "\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, \n",
    "                                              y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb26f12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b'@ menyrbie @ phil_gahan @ chrisitv http : //t.co/ifz9fan2pa http : //t.co/xx6ghgfzcc http : //t.co/i2nlzdxno8'\n",
      "label:  [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# veo un dato de train\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a546b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b'@ menyrbie @ phil_gahan @ chrisitv http : //t.co/ifz9fan2pa http : //t.co/xx6ghgfzcc http : //t.co/i2nlzdxno8'\n",
      "label:  [0. 1. 0.]\n",
      "text:  b'advice talk neighbour family exchange phone number create contact list phone number neighbour school employer chemist gp set online shopping account po adequate supply regular med order'\n",
      "label:  [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# veo un dato de train\n",
    "for example, label in train_dataset.take(2):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbf9b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35534a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb81572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'economic impact low income family taking toll issue shared today driver small company laid work w 4 mouth feed grocery price rising one watch kid need work employer pay'\n",
      " b'u complaining school shut u supermarket nh staff come work hardly choice basically get verbally abused stock best'\n",
      " b'@ menyrbie @ phil_gahan @ chrisitv http : //t.co/ifz9fan2pa http : //t.co/xx6ghgfzcc http : //t.co/i2nlzdxno8']\n",
      "\n",
      "labels: , [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print(f'texts:  {example.numpy()[:3]}\\n')\n",
    "  print(f'labels: , {label.numpy()[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131d54e",
   "metadata": {},
   "source": [
    "<a name=\"4.8\"></a>\n",
    "### <b> <font color='#5499C7'> 4.8. Text Vectorization layer </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c235e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "max_length = 45 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n",
    "\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                    output_mode=\"int\")\n",
    "                                    #output_sequence_length=max_length)\n",
    "\n",
    "# Fit the text vectorizer instance to the training data using the adapt() method\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c28e055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'http', 'coronavirus', 'covid19', 'price', 'store',\n",
       "       'supermarket', 'food', 'grocery', 'people', 'amp', 'consumer',\n",
       "       '19', 'covid', 'shopping', 's', 'online', 'need', 'pandemic'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c51c011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 223,   94,  138,  672,  107,  192, 2037,  327, 1787,   62,  209,\n",
       "         243,   92, 1459,   38,  461,  197, 1476,  860,    9,    5,  772,\n",
       "          34,  419,  396,   18,   38, 1850,  183,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [  23, 3041,  390,  540,   23,    7,  301,   81,  147,   38, 3716,\n",
       "        1143, 2099,   21, 5616, 4312,   33,  224,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [   1,    1,    1,    2,    1,    2,    1,    2,    1,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08d49f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  b'economic impact low income family taking toll issue shared today driver small company laid work w 4 mouth feed grocery price rising one watch kid need work employer pay'\n",
      "Round-trip:  economic impact low income family taking toll issue shared today driver small company laid work w 4 mouth feed grocery price rising one watch kid need work employer pay          \n",
      "\n",
      "Original:  b'u complaining school shut u supermarket nh staff come work hardly choice basically get verbally abused stock best'\n",
      "Round-trip:  u complaining school shut u supermarket nh staff come work hardly choice basically get verbally abused stock best                     \n",
      "\n",
      "Original:  b'@ menyrbie @ phil_gahan @ chrisitv http : //t.co/ifz9fan2pa http : //t.co/xx6ghgfzcc http : //t.co/i2nlzdxno8'\n",
      "Round-trip:  [UNK] [UNK] [UNK] http [UNK] http [UNK] http [UNK]                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d886b2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x740d6f9c4e50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, # set the input shape; size of our vocabulary\n",
    "                                 output_dim=128, # set the size of the embedding vector\n",
    "                                 embeddings_initializer=\"uniform\", # default, initialize embedding vectors randomly\n",
    "                                 input_length=max_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690a7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8518c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,callbacks,models,Sequential,losses\n",
    "\n",
    "INPUT_SHAPE=(1,)\n",
    "\n",
    "def build_model_1(input_shape,name):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=tf.string) # inputs are 1-dimensional strings\n",
    "    x = encoder(inputs) # turn the input text into numbers \n",
    "    x = embedding(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x) # dsp.  num_classes\n",
    "    model = tf.keras.Model(inputs, outputs, name=name) # construct the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model_1(INPUT_SHAPE,'model_1')\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_1 = model_1.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        verbose=0,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400cf9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score1 = model_1.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbcad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
