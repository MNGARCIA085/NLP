{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8836603",
   "metadata": {},
   "source": [
    "# <center><font color='blue'>SENTIMENT ANALYSIS: COVID</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346ea36",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [1 - Objetivos](#1)\n",
    "- [2 - Librerías necesarias](#2)\n",
    "- [3 - Carga y visualización de datos](#3)\n",
    "- [4 - Pre-procesamiento de datos](#4)\n",
    "    - [4.1. - Datos faltantes](#4.1)\n",
    "    - [4.2. - Data Categóricos](#4.2)\n",
    "    - [4.3. - Balanceo de clases](#4.3)\n",
    "    - [4.4. - Pre-Procesamiento especial para NLP](#4.4)\n",
    "- [5 - Modelos](#5)\n",
    "    - [5.1. - Modelo 1](#5.1)\n",
    "    - [5.2. - Modelo 2](#5.2)\n",
    "    - [5.3. - Modelo 3](#5.3)\n",
    "    - [5.4. - Modelo 4](#5.4)\n",
    "    - [5.4. - Modelo 5](#5.5)\n",
    "    - [5.4. - Modelo 6](#5.6)\n",
    "    - [5.4. - Modelo 7](#5.7)\n",
    "- [6 - Comparando los modelos y eligiendo el mejor](#6)\n",
    "- [7 - Ajuste de hiperparámetros](#7)\n",
    "- [8 - Predicciones con el modelo final](#8)\n",
    "- [9 - Guardando el modelo](#9)\n",
    "- [10 - Conclusiones](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269be78",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <b> <font color='blue'> 1. Objectives </font> </b>\n",
    "\n",
    "Practice with a natural language processing problem.\n",
    "<br>\n",
    "Here, given a set of tweets, analyze whether the sentiment is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a2b89",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <b> <font color='blue'> 2. Setup </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7d0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not print info and warning messages\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7215012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 21:05:02.858003: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 21:05:02.858036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 21:05:02.859219: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c80fbf",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <b> <font color='blue'> 3. Data loading and visualization </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bef8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona_NLP_test.csv  Corona_NLP_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae59800",
   "metadata": {},
   "source": [
    "We can see that we have 2 datasets, one for training and one for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9303df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/Corona_NLP_train.csv',encoding='latin-1')\n",
    "test_data = pd.read_csv('data/Corona_NLP_test.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5707b2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3805</td>\n",
       "      <td>48757</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3807</td>\n",
       "      <td>48759</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                        OriginalTweet  \\\n",
       "0                                                                                                                                                                                                                     @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8   \n",
       "1                                                                                       advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order   \n",
       "2                                                                                                                                                                                                 Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P   \n",
       "3   My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j   \n",
       "4  Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n   \n",
       "5                                                                          As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU   \n",
       "6                                                                                                                                                Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO   \n",
       "7                                                                                                                                                                                                       Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ   \n",
       "8                                            Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i   \n",
       "9                                                         For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona   \n",
       "\n",
       "            Sentiment  \n",
       "0             Neutral  \n",
       "1            Positive  \n",
       "2            Positive  \n",
       "3            Positive  \n",
       "4  Extremely Negative  \n",
       "5            Positive  \n",
       "6            Positive  \n",
       "7             Neutral  \n",
       "8            Positive  \n",
       "9            Negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some of the training data\n",
    "pd.set_option('display.max_colwidth', None) # que No recorte el texto\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba28b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training data: (rows, cols) = (41157, 6)\n",
      " Training data type:  <class 'pandas.core.frame.DataFrame'>\n",
      " Test data: (rows, cols) = (3798, 6)\n",
      " Test data type:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(f\" Training data: (rows, cols) = {train_data.shape}\")\n",
    "print(f\" Training data type:  {type(train_data)}\")\n",
    "print(f\" Test data: (rows, cols) = {test_data.shape}\")\n",
    "print(f\" Test data type:  {type(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83400ac",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <b> <font color='blue'> 4. Data Pre-processing </font> </b>\n",
    "\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### <b> <font color='#5499C7'> 4.1. Handling missing data </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad405f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data train:\n",
      " UserName            0\n",
      "ScreenName          0\n",
      "Location         8590\n",
      "TweetAt             0\n",
      "OriginalTweet       0\n",
      "Sentiment           0\n",
      "dtype: int64 \n",
      "\n",
      "Missing data test:\n",
      " UserName           0\n",
      "ScreenName         0\n",
      "Location         834\n",
      "TweetAt            0\n",
      "OriginalTweet      0\n",
      "Sentiment          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing data train:\\n {train_data.isnull().sum()} \\n')\n",
    "print(f'Missing data test:\\n {test_data.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c612cde",
   "metadata": {},
   "source": [
    "There are no missing data in the columns we care about (Original Tweet and Sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0d368",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### <b> <font color='#5499C7'> 4.2. Separate features from target </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4e1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['OriginalTweet']\n",
    "y_train = train_data['Sentiment']\n",
    "X_test = test_data['OriginalTweet']\n",
    "y_test = test_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "478638af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.series.Series)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017ad9c",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### <b> <font color='#5499C7'> 4.3. Categorical data </font> </b>\n",
    "\n",
    "Our target (Sentiment) is categorical, lets see the different options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c27cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cfecb",
   "metadata": {},
   "source": [
    "Lets see if there are other options in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f1696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Extremely Negative', 'Positive', 'Extremely Positive', 'Negative',\n",
       "       'Neutral'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60725602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are unique values the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.array_equal(np.sort(y_train.unique()), np.sort(y_test.unique()))\n",
    "print(f\"Are unique values the same? {are_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317473",
   "metadata": {},
   "source": [
    "We are not so interested in whether a sentiment is positive or extremely positive, therefore we will rename the label extremely positive to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c029b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace('Extremely Positive', 'Positive').replace('Extremely Negative', 'Negative')\n",
    "y_test = y_test.replace('Extremely Positive', 'Positive').replace('Extremely Negative', 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e675d4",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb173216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive'] ['Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(y_train.unique()), np.sort(y_test.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8ba65",
   "metadata": {},
   "source": [
    "We will not consider that there is an ordinal relationship, therefore we will encode the labels using one-hot encoding.\n",
    "\n",
    "Remember that we need to apply the same encoding to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea80172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded train set:\n",
      "       sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
      "0                     0.0                1.0                 0.0\n",
      "1                     0.0                0.0                 1.0\n",
      "2                     0.0                0.0                 1.0\n",
      "3                     0.0                0.0                 1.0\n",
      "4                     1.0                0.0                 0.0\n",
      "...                   ...                ...                 ...\n",
      "41152                 0.0                1.0                 0.0\n",
      "41153                 1.0                0.0                 0.0\n",
      "41154                 0.0                0.0                 1.0\n",
      "41155                 0.0                1.0                 0.0\n",
      "41156                 1.0                0.0                 0.0\n",
      "\n",
      "[41157 rows x 3 columns]\n",
      "\n",
      "One-hot encoded test set:\n",
      "      sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
      "0                    1.0                0.0                 0.0\n",
      "1                    0.0                0.0                 1.0\n",
      "2                    0.0                0.0                 1.0\n",
      "3                    1.0                0.0                 0.0\n",
      "4                    0.0                1.0                 0.0\n",
      "...                  ...                ...                 ...\n",
      "3793                 0.0                0.0                 1.0\n",
      "3794                 1.0                0.0                 0.0\n",
      "3795                 0.0                1.0                 0.0\n",
      "3796                 1.0                0.0                 0.0\n",
      "3797                 0.0                0.0                 1.0\n",
      "\n",
      "[3798 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the train set\n",
    "encoder.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform both train and test sets\n",
    "one_hot_train = encoder.transform(y_train.values.reshape(-1, 1))\n",
    "one_hot_test = encoder.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Convert to DataFrame for easier inspection\n",
    "y_train = pd.DataFrame(one_hot_train, columns = encoder.get_feature_names_out(['sentiment']))\n",
    "y_test = pd.DataFrame(one_hot_test, columns = encoder.get_feature_names_out(['sentiment']))\n",
    "\n",
    "print(\"One-hot encoded train set:\")\n",
    "print(y_train)\n",
    "print(\"\\nOne-hot encoded test set:\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70e8f0",
   "metadata": {},
   "source": [
    "In this example, handle_unknown='ignore' is used to ensure that any categories present in the test set but not in the train set are handled gracefully. This is useful when you have categories in the test set that were not seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734d477",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### <b> <font color='#5499C7'> 4.4. Train/val split </font> </b>\n",
    "\n",
    "We are going to create a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "815312b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Perform the split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e406379",
   "metadata": {},
   "source": [
    "<a name=\"4.5\"></a>\n",
    "### <b> <font color='#5499C7'> 4.5. Class balance </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "849a581f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
       "0.0                 0.0                1.0                   0.438238\n",
       "1.0                 0.0                0.0                   0.374670\n",
       "0.0                 1.0                0.0                   0.187092\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5e859",
   "metadata": {},
   "source": [
    "We have some imbalance with respect to the neutral class. We are going to apply SMOTE to take care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cfd6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ade68428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom imblearn.over_sampling import SMOTE\\n\\n# Applyt SMOTE only to the training set\\nsmote = SMOTE(random_state=42)\\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\\n\\n# Show the new distribution\\nprint(y_train_resampled.value_counts())\\n\\npara smote debo tener las etiquetas en formato categórico\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Applyt SMOTE only to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Show the new distribution\n",
    "print(y_train_resampled.value_counts())\n",
    "\n",
    "para smote debo tener las etiquetas en formato categórico\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95751e22",
   "metadata": {},
   "source": [
    "<a name=\"4.6\"></a>\n",
    "### <b> <font color='#5499C7'> 4.6. Pre-process for NLP </font> </b>\n",
    "\n",
    "Let's preprocess the text from OriginalTweet, for which:\n",
    "\n",
    "- We will remove stop words.\n",
    "- We will remove certain special characters, like \"@\".\n",
    "- We will apply lemmatization.\n",
    "\n",
    "<b>Note:</b> We will also remove punctuation, convert everything to lowercase, and tokenize later using TextVectorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0b931ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840184a6",
   "metadata": {},
   "source": [
    "Let's try our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bf724a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! sample text .\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello @world! This is# a sample text.\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2bb86",
   "metadata": {},
   "source": [
    "Let's apply ou pre-process function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ff3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = X_train.apply(preprocess_text)\n",
    "X_val_prep = X_val.apply(preprocess_text)\n",
    "X_test_prep = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db328c",
   "metadata": {},
   "source": [
    "Let' see a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ab6c8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07e86884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'menyrbie phil_gahan chrisitv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a60ac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 8191,  3725, 22759, 15010, 39142, 30040, 30389, 18537,   919,\n",
       "            33677,\n",
       "            ...\n",
       "            41090, 16023, 21962, 37194, 16850,  6265, 11284, 38158,   860,\n",
       "            15795],\n",
       "           dtype='int64', length=32925)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51993cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unemployment claims made online in Virginia this week:\\r\\r\\n\\r\\r\\nMonday: 426\\r\\r\\nTuesday: 2,150\\r\\r\\n\\r\\r\\nAnd the numbers are going to get bigger. https://t.co/fUeg2RL2dl'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99d85ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unemployment claim made online virginia week : monday : 426 tuesday : 2,150 number going get bigger .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep[8191]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2816657",
   "metadata": {},
   "source": [
    "### Text Vectorization layer\n",
    "\n",
    "Now we will create and apply a layer called <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\" target='_blank'>TextVectorization</a>, which will remove punctuation, convert everything to lowercase, and tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5782e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "sequence_length = 50 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n",
    "\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length = sequence_length)\n",
    "\n",
    "# Fit the text vectorizer instance to the training data using the adapt() method\n",
    "vectorizer.adapt(X_train_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1edc45",
   "metadata": {},
   "source": [
    "Let' see the 20 firsts tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0abbc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'coronavirus', 'covid19', 'price', 'store',\n",
       "       'supermarket', 'food', 'grocery', 'people', 'amp', 'consumer',\n",
       "       '19', 'covid', 'shopping', 's', 'online', 'need', 'time',\n",
       "       'pandemic'], dtype='<U27')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a41671",
   "metadata": {},
   "source": [
    "We are going to apply this layer to our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6876cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to the datasets\n",
    "X_train_vectors = vectorizer(X_train_prep)\n",
    "X_val_vectors = vectorizer(X_val_prep)\n",
    "X_test_vectors = vectorizer(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb8791bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32925, 50), dtype=int64, numpy=\n",
       "array([[ 821,  743,  232, ...,    0,    0,    0],\n",
       "       [ 269,  189,    3, ...,    0,    0,    0],\n",
       "       [   1, 2136, 5977, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    1, 2221, ...,    0,    0,    0],\n",
       "       [1033,  127, 1425, ...,    0,    0,    0],\n",
       "       [1361, 8579,    2, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8a9fd",
   "metadata": {},
   "source": [
    "<a name=\"4.7\"></a>\n",
    "### <b> <font color='#5499C7'> 4.7. Prepare datasets for TensorFlow </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87ba07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_vectors, \n",
    "                                              y_train.values))\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val_vectors, \n",
    "                                              y_val.values))\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_vectors, \n",
    "                                              y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70fc423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  [ 821  743  232   16 3366   47  806    1 1277    1  297   35   20 2672\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "label:  [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# let's see an example for train\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0ca14",
   "metadata": {},
   "source": [
    "We define the size of the buffer and the size of the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd039738",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1639750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642fdee",
   "metadata": {},
   "source": [
    "Let's see some examples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7602eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [[   1 1216  663  478  801  663  478  285  209  663  478 2913 4714    4\n",
      "   663  478  195   22  225   46    3    2    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  19  766  143  364  868  937  161    3   42  766  143  868  364  169\n",
      "   294   11 1074  131   14   58   78 1902 9468    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  46  879    1    1    7  117  560  329  230   30  385  687   34   46\n",
      "    32  251  882  283 1318 1159   86  151  256    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "\n",
      "labels: , [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print(f'texts:  {example.numpy()[:3]}\\n')\n",
    "  print(f'labels: , {label.numpy()[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b650b",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <b> <font color='blue'> 5. Models </font> </b>\n",
    "\n",
    "Let's try different models; all of them are going to have an embedding layer, that's why we are going to start by creating one of them.\n",
    "\n",
    "Also, we're gonna set the random seed, create a dict to store the results, define a constant with the size of the input and a constant with the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2fc28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "results = {}\n",
    "\n",
    "INPUT_SHAPE=(sequence_length,)\n",
    "\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0403b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorflow.keras import layers,callbacks,models,Sequential,losses\n",
    "\n",
    "# function to store the results\n",
    "from utils import store_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a44476",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### <b> <font color='#5499C7'> 5.1. Embedding layer </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4346897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x7bdc5abf4940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, # set the input shape; size of our vocabulary\n",
    "                                 output_dim=128, # set the size of the embedding vector\n",
    "                                 embeddings_initializer=\"uniform\", # default, initialize embedding vectors randomly\n",
    "                                 input_length=sequence_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e59d94",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### <b> <font color='#5499C7'> 5.2. Compile and fit the models </font> </b>\n",
    "\n",
    "\"Since it will be the same for everyone, we define a function to compile and train the models.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f970e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_fit(model, train_dataset, validation_dataset, \n",
    "                    epochs=5, loss='categorical_crossentropy', optimizer='adam',\n",
    "                    metrics=['accuracy', 'Precision','Recall'], verbose = 1):\n",
    "    # Compile model\n",
    "    model.compile(loss=loss,\n",
    "                    optimizer = optimizer,\n",
    "                    metrics = metrics)\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(train_dataset,\n",
    "                            epochs = epochs,\n",
    "                            verbose = verbose,\n",
    "                            validation_data = validation_dataset)\n",
    "    \n",
    "    # return\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42561e36",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### <b> <font color='#5499C7'> 5.2. Model 1 </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d1a0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape=input_shape) # inputs are 1-dimensional integers\n",
    "    x = embedding(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=name) # construct the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model_1(INPUT_SHAPE, NUM_CLASSES, 'model_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2bbef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280387 (4.88 MB)\n",
      "Trainable params: 1280387 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d56e0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1029/1029 [==============================] - 21s 20ms/step - loss: 0.8785 - accuracy: 0.6111 - precision: 0.8124 - recall: 0.2937 - val_loss: 0.7150 - val_accuracy: 0.7287 - val_precision: 0.8216 - val_recall: 0.5573\n",
      "Epoch 2/5\n",
      "1029/1029 [==============================] - 20s 20ms/step - loss: 0.5877 - accuracy: 0.7890 - precision: 0.8550 - recall: 0.6794 - val_loss: 0.5993 - val_accuracy: 0.7824 - val_precision: 0.8375 - val_recall: 0.7019\n",
      "Epoch 3/5\n",
      "1029/1029 [==============================] - 21s 20ms/step - loss: 0.4604 - accuracy: 0.8492 - precision: 0.8868 - recall: 0.7936 - val_loss: 0.5766 - val_accuracy: 0.7931 - val_precision: 0.8289 - val_recall: 0.7340\n",
      "Epoch 4/5\n",
      "1029/1029 [==============================] - 24s 23ms/step - loss: 0.3896 - accuracy: 0.8762 - precision: 0.9013 - recall: 0.8429 - val_loss: 0.5639 - val_accuracy: 0.8020 - val_precision: 0.8299 - val_recall: 0.7688\n",
      "Epoch 5/5\n",
      "1029/1029 [==============================] - 24s 23ms/step - loss: 0.3430 - accuracy: 0.8940 - precision: 0.9137 - recall: 0.8726 - val_loss: 0.5769 - val_accuracy: 0.8031 - val_precision: 0.8247 - val_recall: 0.7761\n"
     ]
    }
   ],
   "source": [
    "history_1 = compile_fit(model_1, train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e867f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 1ms/step - loss: 0.6032 - accuracy: 0.7894 - precision: 0.8075 - recall: 0.7575\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score1 = model_1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c37bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 1', score1); # \";\" because i do not want to print the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafc535",
   "metadata": {},
   "source": [
    "<a name=\"5.3\"></a>\n",
    "### <b> <font color='#5499C7'> 5.3. Model 2 - LSTM </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=\"float32\")\n",
    "    x = embedding(inputs)\n",
    "    x = layers.LSTM(64, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.5)(x) \n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "    \n",
    "model_2 = build_model_2(INPUT_SHAPE, NUM_CLASSES, 'model_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df37952",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = compile_fit(model_2, train_dataset, validation_dataset, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e765e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score2 = model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 2', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c270a1",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"></a>\n",
    "### <b> <font color='#5499C7'> 5.4. Model 3 - GRU </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce38b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_3(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape=(1,), dtype=\"float32\")\n",
    "    x = embedding(inputs)\n",
    "    x = layers.GRU(64, activation=\"tanh\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model= tf.keras.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = build_model_3(INPUT_SHAPE, NUM_CLASSES, 'model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30017f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_3 = compile_fit(model_3, train_dataset, validation_dataset, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d619b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score3 = model_3.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 3', score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678fe40",
   "metadata": {},
   "source": [
    "<a name=\"5.5\"></a>\n",
    "### <b> <font color='#5499C7'> 5.5. Model 4 - Bidirectional RNN </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_4(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=\"float32\")\n",
    "    x = embedding(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "\n",
    "model_4 = build_model_4(INPUT_SHAPE, NUM_CLASSES, 'model_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeddaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3449fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_4 = compile_fit(model_4, train_dataset, validation_dataset, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score4 = model_4.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 4', score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0678fe3",
   "metadata": {},
   "source": [
    "<a name=\"5.6\"></a>\n",
    "### <b> <font color='#5499C7'> 5.6. Model 5 - Stacking layers </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_5(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape=input_shape, dtype='float32')\n",
    "    x = embedding(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # return_sequences=True to stack layers\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "    \n",
    "\n",
    "model_5 = build_model_5(INPUT_SHAPE, NUM_CLASSES, 'model_5')\n",
    "\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5 = compile_fit(model_5, train_dataset, validation_dataset, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score5 = model_5.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 5', score5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164c9c3",
   "metadata": {},
   "source": [
    "<a name=\"5.7\"></a>\n",
    "### <b> <font color='#5499C7'> 5.7. Model 6 - Conv 1D </font> </b>\n",
    "\n",
    "We've seen before how convolutional neural networks can be used for images but they can also be used for text.\n",
    "\n",
    "Previously we've used the layer Conv2D (which is great for images with (height, width)).\n",
    "\n",
    "But if we want to use convolutional layers for sequences (e.g. text) we need to use Conv1D: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
    "\n",
    "For more of a deep dive into what goes on behind the scenes in a CNN for text (or sequences) see the paper: https://arxiv.org/abs/1809.08037\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_6(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape = input_shape)    \n",
    "    x = embedding(inputs)\n",
    "    x = layers.Conv1D(filters = 32, kernel_size= 5, activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "    \n",
    "    \n",
    "model_6 = build_model_6(INPUT_SHAPE, NUM_CLASSES, 'model_6')\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_6 = compile_fit(model_6, train_dataset, validation_dataset, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92324eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score6 = model_6.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b591884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 6', score6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c062e2d",
   "metadata": {},
   "source": [
    "<a name=\"5.7\"></a>\n",
    "### <b> <font color='#5499C7'> 5.7. Model 7 - TensorFlow Hub Pretrained Sentence Encoder </font> </b>\n",
    "\n",
    "Now we've built a few of our own models, let's try and use transfer learning for NLP, specifically using TensorFlow Hub's Universal Sentence Encoder: https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "\n",
    "See how the USE was created here: https://arxiv.org/abs/1803.11175\n",
    "\n",
    "📖 **Resource:** TensorFlow Hub is a great resource for many pretrained models but HuggingFace is also another incredible resource for many pretrained NLP models (using HuggingFace model is beyond the scope of this course but it is definitely something you should be familiar with in the NLP space): https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86735b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras Layer using the USE pretrained layer from tensorflow hub\n",
    "# it takes care of text vectorization and embedding\n",
    "\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False,\n",
    "                                        name=\"USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/47268608/confusion-about-keras-rnn-input-shape-requirement\n",
    "#INPUT_SHAPE = ()  # Scalar for raw text input\n",
    "\n",
    "\n",
    "def build_model_7(input_shape, num_classes, name):\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string)  # Expect raw text input\n",
    "    x = sentence_encoder_layer(inputs)  # Encode text to embeddings\n",
    "    x = layers.Dense(128, activation='relu')(x)  # Fully connected layer\n",
    "    x = layers.Dropout(0.5)(x)  # Optional dropout layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(x)\n",
    "    model = models.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "\n",
    "model_7 = build_model_7(INPUT_SHAPE, NUM_CLASSES, 'model_7')\n",
    "\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e36ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for this model (remember this model does text vectorization)\n",
    "train_dataset2 = tf.data.Dataset.from_tensor_slices((X_train_prep, \n",
    "                                              y_train.values))\n",
    "\n",
    "\n",
    "validation_dataset2 = tf.data.Dataset.from_tensor_slices((X_val_prep, \n",
    "                                              y_val.values))\n",
    "\n",
    "\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((X_test_prep, \n",
    "                                              y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset2 = train_dataset2.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset2 = validation_dataset2.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset2 = test_dataset2.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7353f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_7 = compile_fit(model_7, train_dataset2, validation_dataset2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "score7 = model_7.evaluate(test_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ee7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "store_results(results, 'Model 7', score7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b4264",
   "metadata": {},
   "source": [
    "<a name=\"5.7\"></a>\n",
    "### <b> <font color='#5499C7'> 5.7. Choosing the best model </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95354da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to DataFrame\n",
    "df_metrics = pd.DataFrame(results).T  # .T transposes the DataFrame for easier viewing\n",
    "df_metrics.reset_index(inplace=True)\n",
    "df_metrics.rename(columns={'index': 'Model'}, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f575a13",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36914cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "models = list(results.keys())\n",
    "metrics = list(results[models[0]].keys())\n",
    "values = {metric: [] for metric in metrics}\n",
    "\n",
    "for model in models:\n",
    "    for metric in metrics:\n",
    "        values[metric].append(results[model][metric])\n",
    "\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i * width, values[metric], width, label=metric)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Models', fontweight='bold')\n",
    "ax.set_ylabel('Value', fontweight='bold')\n",
    "ax.set_title('Comparison of Models by Metrics', fontweight='bold', color='#12222C')\n",
    "ax.set_xticks(x + width * (len(metrics) / 2 - 0.5))\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c25c0e",
   "metadata": {},
   "source": [
    "Now, let's view each metric separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5476260",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(metrics), 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results[model][metric] for model in models]\n",
    "    axs[i].bar(models, values, color=['b', 'g', 'r'])\n",
    "    axs[i].set_title(f'{metric.capitalize()} Comparison', fontweight='bold', color='#1B4F72')\n",
    "    axs[i].set_ylabel('Value', fontweight='bold')\n",
    "\n",
    "axs[-1].set_xlabel('Models', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2920d",
   "metadata": {},
   "source": [
    "To choose the best model, we will use the F1-score as our metric.\n",
    "\n",
    "It should be noted that this is a simplification, as we are only training for 5 epochs and not considering other factors, such as which models are more prone to overfitting, which ones take longer to train and infer, etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the row with the highest F1 score\n",
    "highest_f1_index = df_metrics['f1_score'].idxmax()\n",
    "\n",
    "# Get the model with the highest F1 score\n",
    "best_model = df_metrics.loc[highest_f1_index]\n",
    "\n",
    "print(f\"Model with the highest F1 score:\\n{best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9a935",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## <b> <font color='blue'> 6. Hyperparamter Tunning </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f404c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
