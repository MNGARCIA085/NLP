{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8836603",
   "metadata": {},
   "source": [
    "# <center><font color='blue'>SENTIMENT ANALYSIS: COVID</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346ea36",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [1 - Objetivos](#1)\n",
    "- [2 - Librerías necesarias](#2)\n",
    "- [3 - Carga y visualización de datos](#3)\n",
    "- [4 - Pre-procesamiento de datos](#4)\n",
    "    - [4.1. - Datos faltantes](#4.1)\n",
    "    - [4.2. - Data Categóricos](#4.2)\n",
    "    - [4.3. - Balanceo de clases](#4.3)\n",
    "    - [4.4. - Pre-Procesamiento especial para NLP](#4.4)\n",
    "- [5 - Modelos](#5)\n",
    "    - [5.1. - Modelo 1](#5.1)\n",
    "    - [5.2. - Modelo 2](#5.2)\n",
    "    - [5.3. - Modelo 3](#5.3)\n",
    "    - [5.4. - Modelo 4](#5.4)\n",
    "    - [5.4. - Modelo 5](#5.5)\n",
    "    - [5.4. - Modelo 6](#5.6)\n",
    "    - [5.4. - Modelo 7](#5.7)\n",
    "- [6 - Comparando los modelos y eligiendo el mejor](#6)\n",
    "- [7 - Ajuste de hiperparámetros](#7)\n",
    "- [8 - Predicciones con el modelo final](#8)\n",
    "- [9 - Guardando el modelo](#9)\n",
    "- [10 - Conclusiones](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269be78",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <b> <font color='blue'> 1. Objectives </font> </b>\n",
    "\n",
    "Practice with a natural language processing problem.\n",
    "<br>\n",
    "Here, given a set of tweets, analyze whether the sentiment is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a2b89",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <b> <font color='blue'> 2. Setup </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7d0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not print info and warning messages\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7215012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 17:53:16.979024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 17:53:16.979061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 17:53:16.980295: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c80fbf",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <b> <font color='blue'> 3. Data loading and visualization </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bef8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona_NLP_test.csv  Corona_NLP_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae59800",
   "metadata": {},
   "source": [
    "We can see that we have 2 datasets, one for training and one for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9303df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/Corona_NLP_train.csv',encoding='latin-1')\n",
    "test_data = pd.read_csv('data/Corona_NLP_test.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5707b2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3805</td>\n",
       "      <td>48757</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3807</td>\n",
       "      <td>48759</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName                   Location     TweetAt  \\\n",
       "0      3799       48751                     London  16-03-2020   \n",
       "1      3800       48752                         UK  16-03-2020   \n",
       "2      3801       48753                  Vagabonds  16-03-2020   \n",
       "3      3802       48754                        NaN  16-03-2020   \n",
       "4      3803       48755                        NaN  16-03-2020   \n",
       "5      3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6      3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7      3806       48758                    Austria  16-03-2020   \n",
       "8      3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9      3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                        OriginalTweet  \\\n",
       "0                                                                                                                                                                                                                     @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8   \n",
       "1                                                                                       advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order   \n",
       "2                                                                                                                                                                                                 Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P   \n",
       "3   My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j   \n",
       "4  Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n   \n",
       "5                                                                          As news of the regionÂs first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU   \n",
       "6                                                                                                                                                Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I'm in Civics class so I know what I'm talking about\". https://t.co/ieFDNeHgDO   \n",
       "7                                                                                                                                                                                                       Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ   \n",
       "8                                            Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i   \n",
       "9                                                         For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona   \n",
       "\n",
       "            Sentiment  \n",
       "0             Neutral  \n",
       "1            Positive  \n",
       "2            Positive  \n",
       "3            Positive  \n",
       "4  Extremely Negative  \n",
       "5            Positive  \n",
       "6            Positive  \n",
       "7             Neutral  \n",
       "8            Positive  \n",
       "9            Negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some of the training data\n",
    "pd.set_option('display.max_colwidth', None) # que No recorte el texto\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba28b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training data: (rows, cols) = (41157, 6)\n",
      " Training data type:  <class 'pandas.core.frame.DataFrame'>\n",
      " Test data: (rows, cols) = (3798, 6)\n",
      " Test data type:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(f\" Training data: (rows, cols) = {train_data.shape}\")\n",
    "print(f\" Training data type:  {type(train_data)}\")\n",
    "print(f\" Test data: (rows, cols) = {test_data.shape}\")\n",
    "print(f\" Test data type:  {type(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83400ac",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <b> <font color='blue'> 4. Data Pre-processing </font> </b>\n",
    "\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### <b> <font color='#5499C7'> 4.1. Handling missing data </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad405f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data train:\n",
      " UserName            0\n",
      "ScreenName          0\n",
      "Location         8590\n",
      "TweetAt             0\n",
      "OriginalTweet       0\n",
      "Sentiment           0\n",
      "dtype: int64 \n",
      "\n",
      "Missing data test:\n",
      " UserName           0\n",
      "ScreenName         0\n",
      "Location         834\n",
      "TweetAt            0\n",
      "OriginalTweet      0\n",
      "Sentiment          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing data train:\\n {train_data.isnull().sum()} \\n')\n",
    "print(f'Missing data test:\\n {test_data.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c612cde",
   "metadata": {},
   "source": [
    "There are no missing data in the columns we care about (Original Tweet and Sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0d368",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### <b> <font color='#5499C7'> 4.2. Separate features from target </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4e1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['OriginalTweet']\n",
    "y_train = train_data['Sentiment']\n",
    "X_test = test_data['OriginalTweet']\n",
    "y_test = test_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "478638af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, pandas.core.series.Series)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017ad9c",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### <b> <font color='#5499C7'> 4.3. Categorical data </font> </b>\n",
    "\n",
    "Our target (Sentiment) is categorical, lets see the different options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c27cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cfecb",
   "metadata": {},
   "source": [
    "Lets see if there are other options in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f1696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Extremely Negative', 'Positive', 'Extremely Positive', 'Negative',\n",
       "       'Neutral'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60725602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are unique values the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.array_equal(np.sort(y_train.unique()), np.sort(y_test.unique()))\n",
    "print(f\"Are unique values the same? {are_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317473",
   "metadata": {},
   "source": [
    "We are not so interested in whether a sentiment is positive or extremely positive, therefore we will rename the label extremely positive to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c029b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace('Extremely Positive', 'Positive').replace('Extremely Negative', 'Negative')\n",
    "y_test = y_test.replace('Extremely Positive', 'Positive').replace('Extremely Negative', 'Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e675d4",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb173216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Neutral' 'Positive'] ['Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(y_train.unique()), np.sort(y_test.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8ba65",
   "metadata": {},
   "source": [
    "We will not consider that there is an ordinal relationship, therefore we will encode the labels using one-hot encoding.\n",
    "\n",
    "Remember that we need to apply the same encoding to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea80172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded train set:\n",
      "       sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
      "0                     0.0                1.0                 0.0\n",
      "1                     0.0                0.0                 1.0\n",
      "2                     0.0                0.0                 1.0\n",
      "3                     0.0                0.0                 1.0\n",
      "4                     1.0                0.0                 0.0\n",
      "...                   ...                ...                 ...\n",
      "41152                 0.0                1.0                 0.0\n",
      "41153                 1.0                0.0                 0.0\n",
      "41154                 0.0                0.0                 1.0\n",
      "41155                 0.0                1.0                 0.0\n",
      "41156                 1.0                0.0                 0.0\n",
      "\n",
      "[41157 rows x 3 columns]\n",
      "\n",
      "One-hot encoded test set:\n",
      "      sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
      "0                    1.0                0.0                 0.0\n",
      "1                    0.0                0.0                 1.0\n",
      "2                    0.0                0.0                 1.0\n",
      "3                    1.0                0.0                 0.0\n",
      "4                    0.0                1.0                 0.0\n",
      "...                  ...                ...                 ...\n",
      "3793                 0.0                0.0                 1.0\n",
      "3794                 1.0                0.0                 0.0\n",
      "3795                 0.0                1.0                 0.0\n",
      "3796                 1.0                0.0                 0.0\n",
      "3797                 0.0                0.0                 1.0\n",
      "\n",
      "[3798 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the train set\n",
    "encoder.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform both train and test sets\n",
    "one_hot_train = encoder.transform(y_train.values.reshape(-1, 1))\n",
    "one_hot_test = encoder.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Convert to DataFrame for easier inspection\n",
    "y_train = pd.DataFrame(one_hot_train, columns = encoder.get_feature_names_out(['sentiment']))\n",
    "y_test = pd.DataFrame(one_hot_test, columns = encoder.get_feature_names_out(['sentiment']))\n",
    "\n",
    "print(\"One-hot encoded train set:\")\n",
    "print(y_train)\n",
    "print(\"\\nOne-hot encoded test set:\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70e8f0",
   "metadata": {},
   "source": [
    "In this example, handle_unknown='ignore' is used to ensure that any categories present in the test set but not in the train set are handled gracefully. This is useful when you have categories in the test set that were not seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734d477",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### <b> <font color='#5499C7'> 4.4. Train/val split </font> </b>\n",
    "\n",
    "We are going to create a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "815312b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Perform the split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e406379",
   "metadata": {},
   "source": [
    "<a name=\"4.5\"></a>\n",
    "### <b> <font color='#5499C7'> 4.5. Class balance </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "849a581f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_Negative  sentiment_Neutral  sentiment_Positive\n",
       "0.0                 0.0                1.0                   0.438238\n",
       "1.0                 0.0                0.0                   0.374670\n",
       "0.0                 1.0                0.0                   0.187092\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5e859",
   "metadata": {},
   "source": [
    "We have some imbalance with respect to the neutral class. We are going to apply SMOTE to take care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cfd6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ade68428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom imblearn.over_sampling import SMOTE\\n\\n# Applyt SMOTE only to the training set\\nsmote = SMOTE(random_state=42)\\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\\n\\n# Show the new distribution\\nprint(y_train_resampled.value_counts())\\n\\npara smote debo tener las etiquetas en formato categórico\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Applyt SMOTE only to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Show the new distribution\n",
    "print(y_train_resampled.value_counts())\n",
    "\n",
    "para smote debo tener las etiquetas en formato categórico\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95751e22",
   "metadata": {},
   "source": [
    "<a name=\"4.6\"></a>\n",
    "### <b> <font color='#5499C7'> 4.6. Pre-process for NLP </font> </b>\n",
    "\n",
    "Let's preprocess the text from OriginalTweet, for which:\n",
    "\n",
    "- We will remove stop words.\n",
    "- We will remove certain special characters, like \"@\".\n",
    "- We will apply lemmatization.\n",
    "\n",
    "\n",
    "<b>Note:</b> We will also remove punctuation, convert everything to lowercase, and tokenize later using TextVectorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf9a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d3226",
   "metadata": {},
   "source": [
    "We will remove the stop words, some special characters and apply lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb2d0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Inicializar lematizador\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fa9ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove specific characters using regex\n",
    "    text = re.sub(r'[@#]', '', text)\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840184a6",
   "metadata": {},
   "source": [
    "Let's try our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bf724a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! sample text .\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello @world! This is# a sample text.\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2bb86",
   "metadata": {},
   "source": [
    "Let's apply ou pre-process function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43ff3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = X_train.apply(preprocess_text)\n",
    "X_val_prep = X_val.apply(preprocess_text)\n",
    "X_test_prep = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db328c",
   "metadata": {},
   "source": [
    "Let' see a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ab6c8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07e86884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'menyrbie phil_gahan chrisitv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a60ac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 8191,  3725, 22759, 15010, 39142, 30040, 30389, 18537,   919,\n",
       "            33677,\n",
       "            ...\n",
       "            41090, 16023, 21962, 37194, 16850,  6265, 11284, 38158,   860,\n",
       "            15795],\n",
       "           dtype='int64', length=32925)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51993cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unemployment claims made online in Virginia this week:\\r\\r\\n\\r\\r\\nMonday: 426\\r\\r\\nTuesday: 2,150\\r\\r\\n\\r\\r\\nAnd the numbers are going to get bigger. https://t.co/fUeg2RL2dl'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[8191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99d85ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unemployment claim made online virginia week : monday : 426 tuesday : 2,150 number going get bigger .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep[8191]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2816657",
   "metadata": {},
   "source": [
    "### Text Vectorization layer\n",
    "\n",
    "Now we will create and apply a layer called <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\" target='_blank'>TextVectorization</a>, which will remove punctuation, convert everything to lowercase, and tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5782e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "max_length = 45 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n",
    "\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                    output_mode=\"int\")\n",
    "                                    #output_sequence_length=max_length)\n",
    "\n",
    "# Fit the text vectorizer instance to the training data using the adapt() method\n",
    "vectorizer.adapt(X_train_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1edc45",
   "metadata": {},
   "source": [
    "Let' see the 20 firsts tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0abbc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'coronavirus', 'covid19', 'price', 'store',\n",
       "       'supermarket', 'food', 'grocery', 'people', 'amp', 'consumer',\n",
       "       '19', 'covid', 'shopping', 's', 'online', 'need', 'time',\n",
       "       'pandemic'], dtype='<U29')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(vectorizer.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a41671",
   "metadata": {},
   "source": [
    "We are going to apply this layer to our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6876cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta capa la aplicaremos luego en el modelo (al inicio) -> versión original\n",
    "\n",
    "\n",
    "# apply to the datasets\n",
    "X_train_vectors = vectorizer(X_train_prep)\n",
    "X_val_vectors = vectorizer(X_val_prep)\n",
    "X_test_vectors = vectorizer(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb8791bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32925, 46), dtype=int64, numpy=\n",
       "array([[ 822,  744,  232, ...,    0,    0,    0],\n",
       "       [ 281,  190,    3, ...,    0,    0,    0],\n",
       "       [   1, 2114, 5896, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [9959,    1, 2203, ...,    0,    0,    0],\n",
       "       [1026,  124, 1416, ...,    0,    0,    0],\n",
       "       [1344, 8453,    2, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8a9fd",
   "metadata": {},
   "source": [
    "<a name=\"4.7\"></a>\n",
    "### <b> <font color='#5499C7'> 4.7. Prepare datasets for TensorFlow </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87ba07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_vectors, \n",
    "                                              y_train.values))\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val_vectors, \n",
    "                                              y_val.values))\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_vectors, \n",
    "                                              y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70fc423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  [ 822  744  232   16 3337   45  802    1 1261    1  294   35   20 2686\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "label:  [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# let's see an example for train\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0ca14",
   "metadata": {},
   "source": [
    "We define the size of the buffer and the size of the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd039738",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1639750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642fdee",
   "metadata": {},
   "source": [
    "Let's see some examples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7602eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [[ 553  159  251 1925 1149 2191  117  357   25    6   61    7   34  121\n",
      "    53 2235  391 1774  131    9 2017  207 2082 1359 1898    1  946  125\n",
      "  1807    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   1 1858   32   93 2206   42 3254   11  562  177    3 1534  134 3081\n",
      "   898 5438    1  562   44   20  777   32    1    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 557    1 4661  370  185  115   62 4661  107    1 7153 1465  339  415\n",
      "    23 1784  318 7499  324    8    5    2   46 1287    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "\n",
      "labels: , [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print(f'texts:  {example.numpy()[:3]}\\n')\n",
    "  print(f'labels: , {label.numpy()[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b650b",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <b> <font color='blue'> 5. Models </font> </b>\n",
    "\n",
    "Let's try different models; all of them are going to have an embedding layer, that's why we are going to start by creating one of them.\n",
    "\n",
    "<a name=\"5.1\"></a>\n",
    "### <b> <font color='#5499C7'> 5.1. Embedding layer </font> </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4346897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x72d10ca8b400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, # set the input shape; size of our vocabulary\n",
    "                                 output_dim=128, # set the size of the embedding vector\n",
    "                                 embeddings_initializer=\"uniform\", # default, initialize embedding vectors randomly\n",
    "                                 input_length=max_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42561e36",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### <b> <font color='#5499C7'> 5.2. Model 1 </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d1a0a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1029/1029 [==============================] - 17s 16ms/step - loss: 0.8708 - accuracy: 0.6138 - precision: 0.8044 - recall: 0.3036 - val_loss: 0.7057 - val_accuracy: 0.7230 - val_precision: 0.8049 - val_recall: 0.5820\n",
      "Epoch 2/5\n",
      "1029/1029 [==============================] - 17s 16ms/step - loss: 0.5812 - accuracy: 0.7906 - precision: 0.8552 - recall: 0.6865 - val_loss: 0.5935 - val_accuracy: 0.7861 - val_precision: 0.8343 - val_recall: 0.7136\n",
      "Epoch 3/5\n",
      "1029/1029 [==============================] - 17s 17ms/step - loss: 0.4555 - accuracy: 0.8518 - precision: 0.8860 - recall: 0.7998 - val_loss: 0.5669 - val_accuracy: 0.8034 - val_precision: 0.8332 - val_recall: 0.7619\n",
      "Epoch 4/5\n",
      "1029/1029 [==============================] - 16s 16ms/step - loss: 0.3870 - accuracy: 0.8787 - precision: 0.9022 - recall: 0.8481 - val_loss: 0.5777 - val_accuracy: 0.8031 - val_precision: 0.8267 - val_recall: 0.7749\n",
      "Epoch 5/5\n",
      "1029/1029 [==============================] - 16s 16ms/step - loss: 0.3423 - accuracy: 0.8940 - precision: 0.9118 - recall: 0.8722 - val_loss: 0.5980 - val_accuracy: 0.8021 - val_precision: 0.8196 - val_recall: 0.7819\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers,callbacks,models,Sequential,losses\n",
    "\n",
    "INPUT_SHAPE=(1,)\n",
    "\n",
    "def build_model_1(input_shape,name):\n",
    "    inputs = layers.Input(shape=input_shape)#, dtype=tf.string) # inputs are 1-dimensional strings\n",
    "    #x = vectorizer(inputs) # turn the input text into numbers\n",
    "    x = embedding(inputs)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x) # dsp.  num_classes\n",
    "    model = tf.keras.Model(inputs, outputs, name=name) # construct the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model_1(INPUT_SHAPE,'model_1')\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_1 = model_1.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e867f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 1ms/step - loss: 0.6224 - accuracy: 0.7844 - precision: 0.8031 - recall: 0.7562\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score1 = model_1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6db9ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After embedding: (None, 1, 128)\n",
      "After LSTM cell: (None, 64)\n",
      "Epoch 1/5\n",
      "1029/1029 [==============================] - 30s 27ms/step - loss: 0.3113 - accuracy: 0.8961 - precision: 0.9263 - recall: 0.8686 - val_loss: 0.7012 - val_accuracy: 0.7958 - val_precision: 0.8084 - val_recall: 0.7838\n",
      "Epoch 2/5\n",
      "1029/1029 [==============================] - 28s 27ms/step - loss: 0.2065 - accuracy: 0.9421 - precision: 0.9513 - recall: 0.9322 - val_loss: 0.7786 - val_accuracy: 0.7977 - val_precision: 0.8048 - val_recall: 0.7907\n",
      "Epoch 3/5\n",
      "1029/1029 [==============================] - 29s 28ms/step - loss: 0.1735 - accuracy: 0.9518 - precision: 0.9598 - recall: 0.9438 - val_loss: 0.8525 - val_accuracy: 0.7970 - val_precision: 0.8034 - val_recall: 0.7907\n",
      "Epoch 4/5\n",
      "1029/1029 [==============================] - 27s 26ms/step - loss: 0.1547 - accuracy: 0.9548 - precision: 0.9639 - recall: 0.9479 - val_loss: 0.8051 - val_accuracy: 0.7880 - val_precision: 0.8008 - val_recall: 0.7747\n",
      "Epoch 5/5\n",
      "1029/1029 [==============================] - 27s 26ms/step - loss: 0.1366 - accuracy: 0.9597 - precision: 0.9683 - recall: 0.9520 - val_loss: 0.8051 - val_accuracy: 0.7850 - val_precision: 0.8016 - val_recall: 0.7681\n"
     ]
    }
   ],
   "source": [
    "def build_model_2(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=\"float32\")\n",
    "    #x = encoder(inputs) # text vectorizer\n",
    "    x = embedding(inputs) #x\n",
    "    print(f\"After embedding: {x.shape}\")\n",
    "    x = layers.LSTM(64, activation=\"tanh\")(x)\n",
    "    print(f\"After LSTM cell: {x.shape}\")\n",
    "    x = layers.Dropout(0.5)(x) \n",
    "    x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer to have on top of LSTM layer\n",
    "    outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "    return model\n",
    "    \n",
    "\n",
    "model_2 = build_model_2(INPUT_SHAPE)\n",
    "\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_2 = model_2.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a08d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 1s 5ms/step - loss: 0.8608 - accuracy: 0.7601 - precision: 0.7765 - recall: 0.7420\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score1 = model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e765e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
