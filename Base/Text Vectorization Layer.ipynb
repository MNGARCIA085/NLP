{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aa3b5f",
   "metadata": {},
   "source": [
    "# <font color='#154360'> <b> <center> Text Vectorization Layer </center> </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce01d4",
   "metadata": {},
   "source": [
    "A preprocessing layer which maps text features to integer sequences.\n",
    "\n",
    "<b> tf.keras.layers.TextVectorization </b>(\n",
    "    \n",
    "    max_tokens=None,\n",
    "    \n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    \n",
    "    split='whitespace',\n",
    "    \n",
    "    ngrams=None,\n",
    "    \n",
    "    output_mode='int',\n",
    "    \n",
    "    output_sequence_length=None,\n",
    "    \n",
    "    pad_to_max_tokens=False,\n",
    "    \n",
    "    vocabulary=None,\n",
    "    \n",
    "    idf_weights=None,\n",
    "    \n",
    "    sparse=False,\n",
    "    \n",
    "    ragged=False,\n",
    "    \n",
    "    encoding='utf-8',\n",
    "    \n",
    "    name=None,\n",
    "    \n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5a9ce",
   "metadata": {},
   "source": [
    "It transforms a batch of strings (one example = one string) into either:\n",
    "\n",
    "- A list of token indices (one example = 1D tensor of integer token indices) \n",
    "- Or a dense representation (one example = 1D tensor of float values representing data about the example's tokens). \n",
    "\n",
    "\n",
    "The vocabulary for the layer must be either:\n",
    "\n",
    "- Supplied on construction \n",
    "- Or learned via adapt(). \n",
    "\n",
    "When this layer is adapted:\n",
    "\n",
    "- It will analyze the dataset.\n",
    "- Determine the frequency of individual string values\n",
    "- Create a vocabulary from them. \n",
    "\n",
    "This vocabulary can have unlimited size or be capped, depending on the configuration options for this layer; if there are more unique values in the input than the maximum vocabulary size, the most frequent terms will be used to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b31d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d5de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 5000  # Maximum vocab size.\n",
    "max_len = 5  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e22f7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the vocab layer has been created, call `adapt` on the\n",
    "# list of strings to create the vocabulary.\n",
    "vectorize_layer.adapt([\"foo bar\", \"bar baz\", \"baz bada boom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405deec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization_1',\n",
       " 'trainable': True,\n",
       " 'dtype': 'string',\n",
       " 'batch_input_shape': (None,),\n",
       " 'max_tokens': 5000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 5,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None,\n",
       " 'encoding': 'utf-8',\n",
       " 'vocabulary_size': 7}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "vectorize_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d88f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'baz', 'bar', 'foo', 'boom', 'bada'], dtype='<U5')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the tokens\n",
    "vocab = np.array(vectorize_layer.get_vocabulary())\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc30aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID a Token: {0: '', 1: '[UNK]', 2: 'baz', 3: 'bar', 4: 'foo', 5: 'boom', 6: 'bada'}\n",
      "Token a ID: {'': 0, '[UNK]': 1, 'baz': 2, 'bar': 3, 'foo': 4, 'boom': 5, 'bada': 6}\n"
     ]
    }
   ],
   "source": [
    "# Dict with the id-token map\n",
    "id_to_token = {idx: word for idx, word in enumerate(vocab)}\n",
    "token_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(\"ID a Token:\", id_to_token)\n",
    "print(\"Token a ID:\", token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbb0b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1 [UNK]\n",
      "2 baz\n",
      "3 bar\n",
      "4 foo\n",
      "5 boom\n",
      "6 bada\n"
     ]
    }
   ],
   "source": [
    "# print them in a nicest way\n",
    "for idx, word in enumerate(vocab):\n",
    "    print(idx, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4630f402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       "array([[4, 1, 3, 0, 0],\n",
       "       [1, 2, 0, 0, 0]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, the layer can map strings to integers -- you can use an\n",
    "# embedding layer to map these integers to learned embeddings.\n",
    "input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n",
    "vectorize_layer(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349b881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc8c6c3f",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[TextVectorization Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
